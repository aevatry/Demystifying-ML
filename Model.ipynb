{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6d4c7f2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import transforms, datasets\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import PIL\n",
    "\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3505de07",
   "metadata": {},
   "source": [
    "# Notes\n",
    "###### Referent paper: https://ietresearch.onlinelibrary.wiley.com/doi/full/10.1049/iet-its.2018.5392\n",
    "\n",
    "Need to install the mtcnn \"library\" to be able to pinpoint the correct areas (eyes) for input to the network  \\\n",
    "Github repo link: https://github.com/ipazc/mtcnn/tree/master/mtcnn  \\\n",
    "Before mtcnn: need to do gamma correction (see https://www.mdpi.com/1999-5903/11/5/115)\n",
    "\n",
    "Could have something where train 2 2D CNNs, one with picture of eye, second with optical flow of mouth  \\\n",
    "Explained in paper for gamma correction. \n",
    "\n",
    "Then, architecture is two 2D CNNs (say based on Res-Net10 but need to check) then LSTM layer  \\\n",
    "For LSTM layer, number of frames is recommended to 16 (see referent paper) but could experiment with it  \\\n",
    "Can try: using the 2D CNNs as feature extractors then the LSTM as the classifier  \\\n",
    "For help on LSTM: https://pytorch.org/tutorials/beginner/nlp/sequence_models_tutorial.html  \n",
    "\n",
    "For help on Transfer Learning: https://pytorch.org/tutorials/beginner/transfer_learning_tutorial.html \n",
    "\n",
    "Res-Net is used for transfer learning: initialise the convolution weights using ResNet  \\\n",
    "For help on Res-Net: \n",
    "- https://github.com/harlan-zhao/Face-Detection-Identification  \n",
    "- https://github.com/nullbyte91/computer-vision-tasks-and-algorithms \n",
    "- https://pytorch.org/vision/stable/models.html \n",
    "- https://pytorch.org/blog/introducing-torchvision-new-multi-weight-support-api/\n",
    "\n",
    "\n",
    "<img src=\"Architecture diagram.png\" width=\"300\">\n",
    "<img src=\"Architecture table.png\" width=\"600\">  \n",
    "\n",
    "To use GPUs from Imperial, need to go see : \n",
    "- https://www.imperial.ac.uk/computing/people/csg/guides/hpcomputing/gpucluster/\n",
    "- https://www.imperial.ac.uk/admin-services/ict/self-service/research-support/rcs/get-access/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06a5db21",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Will need to change this so that it accepts different models: this mainly means add a parameter data\n",
    "def train_model(model, criterion, optimizer, scheduler, num_epochs=25):\n",
    "    since = time.time()\n",
    "\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f'Epoch {epoch}/{num_epochs - 1}')\n",
    "        print('-' * 10)\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                model.train()  # Set model to training mode\n",
    "            else:\n",
    "                model.eval()   # Set model to evaluate mode\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "\n",
    "            # Iterate over data.  \n",
    "            \"\"\"This NEED to be changed depending on dataset we use\n",
    "            Because we are gonna use different datasets for each model\"\"\"\n",
    "            for inputs, labels in dataloaders[phase]:  \n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # forward\n",
    "                # track history if only in train\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    outputs = model(inputs)\n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "                    loss = criterion(outputs, labels)\n",
    "\n",
    "                    # backward + optimize only if in training phase\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                # statistics\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "            if phase == 'train':\n",
    "                scheduler.step()\n",
    "\n",
    "            epoch_loss = running_loss / dataset_sizes[phase]\n",
    "            epoch_acc = running_corrects.double() / dataset_sizes[phase]\n",
    "\n",
    "            print(f'{phase} Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}')\n",
    "\n",
    "            # deep copy the model\n",
    "            if phase == 'val' and epoch_acc > best_acc:\n",
    "                best_acc = epoch_acc\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "\n",
    "        print()\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print(f'Training complete in {time_elapsed // 60:.0f}m {time_elapsed % 60:.0f}s')\n",
    "    print(f'Best val Acc: {best_acc:4f}')\n",
    "\n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e0c9b1bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "512\n"
     ]
    }
   ],
   "source": [
    "# Model for feature extraction for the eyes\n",
    "model_efe = torchvision.models.resnet18(weights='DEFAULT')\n",
    "\n",
    "for params in model_fe.parameters():\n",
    "    params.requires_grad = False\n",
    "\n",
    "num_ftrs = model_efe.fc.in_features #the last layer was fully connected 512 to 1000 for initial ResNet 18 \n",
    "# but we are changing this one with the next line of code\n",
    "\n",
    "model_efe.fc = nn.Linear(num_ftrs, 2) #where 2 is the number of labels on the dataset (not sure about the number)\n",
    "# we can also add a fully connected layer in between to extract the number of features we want for LSTM\n",
    "# or have the two outputs as the features for the eye for LSTM \n",
    "\n",
    "criterion_efe = nn.CrossEntropyLoss()\n",
    "\n",
    "optimizer_efe = optim.Adam(model_efe.fc.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "#decay learning rate by a factor of 0.1 every 7 epochs\n",
    "efe_lr_scheduler = lr_scheduler.StepLR(optimizer_efe, step_size=7, gamma=0.1) \n",
    "\n",
    "#model_efe = train_model(model_efe, criterion_efe, optimizer_efe, efe_lr_scheduler,num_epochs=25)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e75ff609",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "class Mouth_Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels = 2, out_channels = 32, kernel_size = 5, padding = 2)\n",
    "        # reason for 2 channels even if have grayscale image is because there would \n",
    "        \n",
    "        #we want 32 neurons in hidden layer\n",
    "        self.conv2 = nn.Conv2d(in_channels = 32, out_channels = 64, kernel_size = 5, padding = 2)\n",
    "        \n",
    "        self.fc1 = nn.Linear(64*7*7, 128)\n",
    "        self.fc2 = nn.Linear(128, 2) # this is to have the same number of features from mouth as eye\n",
    "        \n",
    "    def Convolution(self, X):\n",
    "        X = F.max_pool2d(F.relu(self.conv1(X)), kernel_size = 2)\n",
    "        X = F.max_pool2d(F.relu(self.conv2(X)), kernel_size = 2)     \n",
    "        return X\n",
    "\n",
    "    #use F.relu and not nn.ReLu because would need to initiate the nn.ReLu that calls the F.ReLu anyway in source code  \n",
    "    def forward(self, X):\n",
    "        \n",
    "        X = self.Convolution(X)\n",
    "        X = X.view(-1, 64*7*7) # need to flatten output before linear layers\n",
    "        \n",
    "        X = F.relu(self.fc1(X))\n",
    "        X = self.fc2(X)\n",
    "        return F.softmax(X, dim = 1) # can return something else here to have the raw inputs but still train on the softmax\n",
    "    \n",
    "model_mfe = Mouth_Net()\n",
    "\n",
    "'''Instances below need to be changed to adapt to the current model'''\n",
    "\n",
    "#criterion_mfe = nn.CrossEntropyLoss()\n",
    "\n",
    "#optimizer_mfe = optim.Adam(model_mfe.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "#decay learning rate by a factor of 0.1 every 7 epochs\n",
    "#mfe_lr_scheduler = lr_scheduler.StepLR(optimizer_mfe, step_size=7, gamma=0.1) \n",
    "\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "565a315e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "class Drowsy_LSTM (nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.lstm_layer = nn.LSTM(input_size = 4, hidden_size = 512)\n",
    "        # input_size can change as well as hidden size\n",
    "        self.dropout = nn.Dropout(p=0.1) #can play with this probability of dropout\n",
    "        self.fc1 = nn.Linear(512, 128)\n",
    "        self.fc2 = nn.Linear(128, 2) #classify between drowsy and not drowsy. \n",
    "        \n",
    "    def forward(self, X):\n",
    "        \n",
    "        X = self.lstm_layer(X)\n",
    "        \n",
    "        # need to see if flattening applies to LSTM\n",
    "        #X = X.view(-1, 64*7*7) # need to flatten output before linear layers\n",
    "        \n",
    "        X = self.dropout(X)\n",
    "        X = F.relu(self.fc1(X))\n",
    "        X = self.fc2(X)\n",
    "        return F.softmax(X, dim = 1) # Might need to change the final function but should not be a problem\n",
    "        # this would be the final result of the \n",
    "        \n",
    "model_lstm = Drowsy_LSTM()\n",
    "\n",
    "'''Instances below need to be changed to adapt to the current model'''\n",
    "\n",
    "#criterion_DLSTM = nn.CrossEntropyLoss()\n",
    "\n",
    "#optimizer_DLSTM = optim.Adam(model_DLSTM.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "#decay learning rate by a factor of 0.1 every 7 epochs\n",
    "#DLSTM_lr_scheduler = lr_scheduler.StepLR(optimizer_DLSTM, step_size=7, gamma=0.1) \n",
    "\n",
    "print()\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
